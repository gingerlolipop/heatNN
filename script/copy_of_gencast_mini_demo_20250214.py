# -*- coding: utf-8 -*-
"""Copy of GenCast Mini Demo - 20250214

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-oIhNf4E3GPa-UxDMIYLx7tgSG9Yju5b

# Notation by Jing - Notebook V1.1 20250214

This notebook was adapted from Google DeepMind's `GenCast Mini Demo` notebook. Original version from their [GitHub repository](https://github.com/google-deepmind/graphcast/blob/main/)


**Steps to run this notebook**

(1) Click "Runtime" -> "Change runtime type". Make sure the hardware accelerator is TPU

(2) run the first cell "Upgrade Packages"

(3) Click "Runtime" -> "Restart Session" (If there is a warning window pop up, simply click "Restart Session" on that window and omit this step)

(4) Run all cells

> Copyright 2024 DeepMind Technologies Limited.
>
> Licensed under the Apache License, Version 2.0 (the "License");
> you may not use this file except in compliance with the License.
> You may obtain a copy of the License at
>
>      http://www.apache.org/licenses/LICENSE-2.0
>
> Unless required by applicable law or agreed to in writing, software
> distributed under the License is distributed on an "AS-IS" BASIS,
> WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
> See the License for the specific language governing permissions and
> limitations under the License.

# GenCast Mini Demo

This notebook demonstrates running `GenCast 1p0deg Mini <2019`.

`GenCast 1p0deg Mini <2019` is a GenCast model at 1deg resolution, with 13 pressure levels and a 4 times refined icosahedral mesh. It is trained on ERA5 data from 1979 to 2018, and can be causally evaluated on 2019 and later years.

While other GenCast models are [available](https://github.com/google-deepmind/graphcast/blob/main/README.md), this model has the smallest memory footprint of those provided and is the only one runnable with the freely provided TPUv2-8 configuration in Colab. You can select this configuration in `Runtime>Change Runtime Type`.

**N.B.** The performance of `GenCast 1p0deg Mini <2019` is reasonable but is not representative of the performance of the other GenCast models described in the [README](https://github.com/google-deepmind/graphcast/blob/main/README.md).

To run the other models using Google Cloud Compute, refer to [gencast_demo_cloud_vm.ipynb](https://colab.research.google.com/github/deepmind/graphcast/blob/master/gencast_demo_cloud_vm.ipynb).

# Installation and Initialization
"""

# Commented out IPython magic to ensure Python compatibility.
# @title Upgrade packages (kernel needs to be restarted after running this cell).

# %pip install -U importlib_metadata

# Commented out IPython magic to ensure Python compatibility.
# @title Pip install repo and dependencies

# %pip install --upgrade https://github.com/deepmind/graphcast/archive/master.zip

# @title Imports

import dataclasses
import datetime
import math
from google.cloud import storage
from typing import Optional
import haiku as hk
from IPython.display import HTML
from IPython import display
import ipywidgets as widgets
import jax
import matplotlib
import matplotlib.pyplot as plt
from matplotlib import animation
import numpy as np
import xarray

from graphcast import rollout
from graphcast import xarray_jax
from graphcast import normalization
from graphcast import checkpoint
from graphcast import data_utils
from graphcast import xarray_tree
from graphcast import gencast
from graphcast import denoiser
from graphcast import nan_cleaning

# @title Plotting functions

def select(
    data: xarray.Dataset,
    variable: str,
    level: Optional[int] = None,
    max_steps: Optional[int] = None
    ) -> xarray.Dataset:
  data = data[variable]
  if "batch" in data.dims:
    data = data.isel(batch=0)
  if max_steps is not None and "time" in data.sizes and max_steps < data.sizes["time"]:
    data = data.isel(time=range(0, max_steps))
  if level is not None and "level" in data.coords:
    data = data.sel(level=level)
  return data

def scale(
    data: xarray.Dataset,
    center: Optional[float] = None,
    robust: bool = False,
    ) -> tuple[xarray.Dataset, matplotlib.colors.Normalize, str]:
  vmin = np.nanpercentile(data, (2 if robust else 0))
  vmax = np.nanpercentile(data, (98 if robust else 100))
  if center is not None:
    diff = max(vmax - center, center - vmin)
    vmin = center - diff
    vmax = center + diff
  return (data, matplotlib.colors.Normalize(vmin, vmax),
          ("RdBu_r" if center is not None else "viridis"))

def plot_data(
    data: dict[str, xarray.Dataset],
    fig_title: str,
    plot_size: float = 5,
    robust: bool = False,
    cols: int = 4
    ) -> tuple[xarray.Dataset, matplotlib.colors.Normalize, str]:

  first_data = next(iter(data.values()))[0]
  max_steps = first_data.sizes.get("time", 1)
  assert all(max_steps == d.sizes.get("time", 1) for d, _, _ in data.values())

  cols = min(cols, len(data))
  rows = math.ceil(len(data) / cols)
  figure = plt.figure(figsize=(plot_size * 2 * cols,
                               plot_size * rows))
  figure.suptitle(fig_title, fontsize=16)
  figure.subplots_adjust(wspace=0, hspace=0)
  figure.tight_layout()

  images = []
  for i, (title, (plot_data, norm, cmap)) in enumerate(data.items()):
    ax = figure.add_subplot(rows, cols, i+1)
    ax.set_xticks([])
    ax.set_yticks([])
    ax.set_title(title)
    im = ax.imshow(
        plot_data.isel(time=0, missing_dims="ignore"), norm=norm,
        origin="lower", cmap=cmap)
    plt.colorbar(
        mappable=im,
        ax=ax,
        orientation="vertical",
        pad=0.02,
        aspect=16,
        shrink=0.75,
        cmap=cmap,
        extend=("both" if robust else "neither"))
    images.append(im)

  def update(frame):
    if "time" in first_data.dims:
      td = datetime.timedelta(microseconds=first_data["time"][frame].item() / 1000)
      figure.suptitle(f"{fig_title}, {td}", fontsize=16)
    else:
      figure.suptitle(fig_title, fontsize=16)
    for im, (plot_data, norm, cmap) in zip(images, data.values()):
      im.set_data(plot_data.isel(time=frame, missing_dims="ignore"))

  ani = animation.FuncAnimation(
      fig=figure, func=update, frames=max_steps, interval=250)
  plt.close(figure.number)
  return HTML(ani.to_jshtml())

"""# Load the Data and initialize the model"""

# @title Authenticate with Google Cloud Storage

# Gives you an authenticated client, in case you want to use a private bucket.
gcs_client = storage.Client.create_anonymous_client()
gcs_bucket = gcs_client.get_bucket("dm_graphcast")
dir_prefix = "gencast/"

"""## Load the model params

Choose one of the two ways of getting model params:
- **checkpoint**: You'll get sensible predictions, but are limited to the model architecture that it was trained with, which may not fit on your device.
- **random**: You'll get random predictions, but you can change the model architecture and data resolution which may run faster or fit on your device.


"""

# @title Choose the model

params_file_options = [
    name for blob in gcs_bucket.list_blobs(prefix=(dir_prefix+"params/"))
    if (name := blob.name.removeprefix(dir_prefix+"params/"))]  # Drop empty string.

latent_value_options = [int(2**i) for i in range(4, 10)]
random_latent_size = widgets.Dropdown(
    options=latent_value_options, value=512,description="Latent size:")
random_attention_type = widgets.Dropdown(
    options=["splash_mha", "triblockdiag_mha", "mha"], value="splash_mha", description="Attention:")
random_mesh_size = widgets.IntSlider(
    value=4, min=4, max=6, description="Mesh size:")
random_num_heads = widgets.Dropdown(
    options=[int(2**i) for i in range(0, 3)], value=4,description="Num heads:")
random_attention_k_hop = widgets.Dropdown(
    options=[int(2**i) for i in range(2, 5)], value=16,description="Attn k hop:")
random_resolution = widgets.Dropdown(
    options=["1p0", "0p25"], value="1p0", description="Resolution:")

def update_latent_options(*args):
  def _latent_valid_for_attn(attn, latent, heads):
    head_dim, rem = divmod(latent, heads)
    if rem != 0:
      return False
    # Required for splash attn.
    if head_dim % 128 != 0:
      return attn != "splash_mha"
    return True
  attn = random_attention_type.value
  heads = random_num_heads.value
  random_latent_size.options = [
      latent for latent in latent_value_options
      if _latent_valid_for_attn(attn, latent, heads)]

# Observe changes to only allow for valid combinations.
random_attention_type.observe(update_latent_options, "value")
random_latent_size.observe(update_latent_options, "value")
random_num_heads.observe(update_latent_options, "value")

params_file = widgets.Dropdown(
    options=[f for f in params_file_options if "Mini" in f],
    description="Params file:",
    layout={"width": "max-content"})

source_tab = widgets.Tab([
    params_file,
    widgets.VBox([
        random_attention_type,
        random_mesh_size,
        random_num_heads,
        random_latent_size,
        random_attention_k_hop,
        random_resolution
    ]),
])
source_tab.set_title(0, "Checkpoint")
source_tab.set_title(1, "Random")
widgets.VBox([
    source_tab,
    widgets.Label(value="Run the next cell to load the model. Rerunning this cell clears your selection.")
])

# @title Load the model

source = source_tab.get_title(source_tab.selected_index)

if source == "Random":
  params = None  # Filled in below
  state = {}
  task_config = gencast.TASK
  # Use default values.
  sampler_config = gencast.SamplerConfig()
  noise_config = gencast.NoiseConfig()
  noise_encoder_config = denoiser.NoiseEncoderConfig()
  # Configure, otherwise use default values.
  denoiser_architecture_config = denoiser.DenoiserArchitectureConfig(
    sparse_transformer_config = denoiser.SparseTransformerConfig(
        attention_k_hop=random_attention_k_hop.value,
        attention_type=random_attention_type.value,
        d_model=random_latent_size.value,
        num_heads=random_num_heads.value
        ),
    mesh_size=random_mesh_size.value,
    latent_size=random_latent_size.value,
  )
else:
  assert source == "Checkpoint"
  with gcs_bucket.blob(dir_prefix + f"params/{params_file.value}").open("rb") as f:
    ckpt = checkpoint.load(f, gencast.CheckPoint)
  params = ckpt.params
  state = {}

  task_config = ckpt.task_config
  sampler_config = ckpt.sampler_config
  noise_config = ckpt.noise_config
  noise_encoder_config = ckpt.noise_encoder_config
  denoiser_architecture_config = ckpt.denoiser_architecture_config
  print("Model description:\n", ckpt.description, "\n")
  print("Model license:\n", ckpt.license, "\n")

"""## Load the example data

Example ERA5 datasets are available at 0.25 degree and 1 degree resolution.

Example HRES-fc0 datasets are available at 0.25 degree resolution.

Some transformations were done from the base datasets:
- We accumulated precipitation over 12 hours instead of the default 1 hour.
- For HRES-fc0 sea surface temperature, we assigned NaNs to grid cells in which sea surface temperature was NaN in the ERA5 dataset (this remains fixed at all times).

The data resolution must match the model that is loaded. Since we are running GenCast Mini, this will be 1 degree.


"""

# @title Get and filter the list of available example datasets

dataset_file_options = [
    name for blob in gcs_bucket.list_blobs(prefix=(dir_prefix + "dataset/"))
    if (name := blob.name.removeprefix(dir_prefix+"dataset/"))]  # Drop empty string.

def parse_file_parts(file_name):
  return dict(part.split("-", 1) for part in file_name.split("_"))


def data_valid_for_model(file_name: str, params_file_name: str):
  """Check data type and resolution matches."""
  data_file_parts = parse_file_parts(file_name.removesuffix(".nc"))
  data_res = data_file_parts["res"].replace(".", "p")
  if source == "Random":
    return random_resolution.value == data_res
  res_matches = data_res in params_file_name.lower()
  source_matches = "Operational" in params_file_name
  if data_file_parts["source"] == "era5":
    source_matches = not source_matches
  return res_matches and source_matches

dataset_file = widgets.Dropdown(
    options=[
        (", ".join([f"{k}: {v}" for k, v in parse_file_parts(option.removesuffix(".nc")).items()]), option)
        for option in dataset_file_options
        if data_valid_for_model(option, params_file.value)
    ],
    description="Dataset file:",
    layout={"width": "max-content"})
widgets.VBox([
    dataset_file,
    widgets.Label(value="Run the next cell to load the dataset. Rerunning this cell clears your selection and refilters the datasets that match your model.")
])

# @title Load weather data

with gcs_bucket.blob(dir_prefix+f"dataset/{dataset_file.value}").open("rb") as f:
  example_batch = xarray.load_dataset(f).compute()

assert example_batch.dims["time"] >= 3  # 2 for input, >=1 for targets

print(", ".join([f"{k}: {v}" for k, v in parse_file_parts(dataset_file.value.removesuffix(".nc")).items()]))

example_batch

# @title Choose data to plot

plot_example_variable = widgets.Dropdown(
    options=example_batch.data_vars.keys(),
    value="2m_temperature",
    description="Variable")
plot_example_level = widgets.Dropdown(
    options=example_batch.coords["level"].values,
    value=500,
    description="Level")
plot_example_robust = widgets.Checkbox(value=True, description="Robust")
plot_example_max_steps = widgets.IntSlider(
    min=1, max=example_batch.dims["time"], value=example_batch.dims["time"],
    description="Max steps")

widgets.VBox([
    plot_example_variable,
    plot_example_level,
    plot_example_robust,
    plot_example_max_steps,
    widgets.Label(value="Run the next cell to plot the data. Rerunning this cell clears your selection.")
])

# @title Plot example data

plot_size = 7

data = {
    " ": scale(select(example_batch, plot_example_variable.value, plot_example_level.value, plot_example_max_steps.value),
              robust=plot_example_robust.value),
}
fig_title = plot_example_variable.value
if "level" in example_batch[plot_example_variable.value].coords:
  fig_title += f" at {plot_example_level.value} hPa"

plot_data(data, fig_title, plot_size, plot_example_robust.value)

# @title Extract training and eval data

train_inputs, train_targets, train_forcings = data_utils.extract_inputs_targets_forcings(
    example_batch, target_lead_times=slice("12h", "12h"), # Only 1AR training.
    **dataclasses.asdict(task_config))

eval_inputs, eval_targets, eval_forcings = data_utils.extract_inputs_targets_forcings(
    example_batch, target_lead_times=slice("12h", f"{(example_batch.dims['time']-2)*12}h"), # All but 2 input frames.
    **dataclasses.asdict(task_config))

print("All Examples:  ", example_batch.dims.mapping)
print("Train Inputs:  ", train_inputs.dims.mapping)
print("Train Targets: ", train_targets.dims.mapping)
print("Train Forcings:", train_forcings.dims.mapping)
print("Eval Inputs:   ", eval_inputs.dims.mapping)
print("Eval Targets:  ", eval_targets.dims.mapping)
print("Eval Forcings: ", eval_forcings.dims.mapping)

# @title Load normalization data

with gcs_bucket.blob(dir_prefix+"stats/diffs_stddev_by_level.nc").open("rb") as f:
  diffs_stddev_by_level = xarray.load_dataset(f).compute()
with gcs_bucket.blob(dir_prefix+"stats/mean_by_level.nc").open("rb") as f:
  mean_by_level = xarray.load_dataset(f).compute()
with gcs_bucket.blob(dir_prefix+"stats/stddev_by_level.nc").open("rb") as f:
  stddev_by_level = xarray.load_dataset(f).compute()
with gcs_bucket.blob(dir_prefix+"stats/min_by_level.nc").open("rb") as f:
  min_by_level = xarray.load_dataset(f).compute()

# @title Build jitted functions, and possibly initialize random weights


def construct_wrapped_gencast():
  """Constructs and wraps the GenCast Predictor."""
  predictor = gencast.GenCast(
      sampler_config=sampler_config,
      task_config=task_config,
      denoiser_architecture_config=denoiser_architecture_config,
      noise_config=noise_config,
      noise_encoder_config=noise_encoder_config,
  )

  predictor = normalization.InputsAndResiduals(
      predictor,
      diffs_stddev_by_level=diffs_stddev_by_level,
      mean_by_level=mean_by_level,
      stddev_by_level=stddev_by_level,
  )

  predictor = nan_cleaning.NaNCleaner(
      predictor=predictor,
      reintroduce_nans=True,
      fill_value=min_by_level,
      var_to_clean='sea_surface_temperature',
  )

  return predictor


@hk.transform_with_state
def run_forward(inputs, targets_template, forcings):
  predictor = construct_wrapped_gencast()
  return predictor(inputs, targets_template=targets_template, forcings=forcings)


@hk.transform_with_state
def loss_fn(inputs, targets, forcings):
  predictor = construct_wrapped_gencast()
  loss, diagnostics = predictor.loss(inputs, targets, forcings)
  return xarray_tree.map_structure(
      lambda x: xarray_jax.unwrap_data(x.mean(), require_jax=True),
      (loss, diagnostics),
  )


def grads_fn(params, state, inputs, targets, forcings):
  def _aux(params, state, i, t, f):
    (loss, diagnostics), next_state = loss_fn.apply(
        params, state, jax.random.PRNGKey(0), i, t, f
    )
    return loss, (diagnostics, next_state)

  (loss, (diagnostics, next_state)), grads = jax.value_and_grad(
      _aux, has_aux=True
  )(params, state, inputs, targets, forcings)
  return loss, diagnostics, next_state, grads


if params is None:
  init_jitted = jax.jit(loss_fn.init)
  params, state = init_jitted(
      rng=jax.random.PRNGKey(0),
      inputs=train_inputs,
      targets=train_targets,
      forcings=train_forcings,
  )


loss_fn_jitted = jax.jit(
    lambda rng, i, t, f: loss_fn.apply(params, state, rng, i, t, f)[0]
)
grads_fn_jitted = jax.jit(grads_fn)
run_forward_jitted = jax.jit(
    lambda rng, i, t, f: run_forward.apply(params, state, rng, i, t, f)[0]
)
# We also produce a pmapped version for running in parallel.
run_forward_pmap = xarray_jax.pmap(run_forward_jitted, dim="sample")

"""# Run the model

The `chunked_prediction_generator_multiple_runs` iterates over forecast steps, where the 1 step forecast is jitted and samples are pmapped across the chips.
This allows us to make efficient use of all devices and parallelise generating an ensemble across them. We then combine the chunks at the end to form our final forecast.

Note that the `Autoregressive rollout` cell will take longer than the standard inference time to run when executed for the first time, as this will include code compilation time. This cost does not increase with the number of devices, it is a fixed-cost one time operation whose result can be reused across any number of devices.
"""

# The number of ensemble members should be a multiple of the number of devices.
print(f"Number of local devices {len(jax.local_devices())}")

# @title Autoregressive rollout (loop in python)

print("Inputs:  ", eval_inputs.dims.mapping)
print("Targets: ", eval_targets.dims.mapping)
print("Forcings:", eval_forcings.dims.mapping)

num_ensemble_members = 8 # @param int
rng = jax.random.PRNGKey(0)
# We fold-in the ensemble member, this way the first N members should always
# match across different runs which use take the same inputs, regardless of
# total ensemble size.
rngs = np.stack(
    [jax.random.fold_in(rng, i) for i in range(num_ensemble_members)], axis=0)

chunks = []
for chunk in rollout.chunked_prediction_generator_multiple_runs(
    # Use pmapped version to parallelise across devices.
    predictor_fn=run_forward_pmap,
    rngs=rngs,
    inputs=eval_inputs,
    targets_template=eval_targets * np.nan,
    forcings=eval_forcings,
    num_steps_per_chunk = 1,
    num_samples = num_ensemble_members,
    pmap_devices=jax.local_devices()
    ):
    chunks.append(chunk)
predictions = xarray.combine_by_coords(chunks)

# @title Choose predictions to plot

plot_pred_variable = widgets.Dropdown(
    options=predictions.data_vars.keys(),
    value="2m_temperature",
    description="Variable")
plot_pred_level = widgets.Dropdown(
    options=predictions.coords["level"].values,
    value=500,
    description="Level")
plot_pred_robust = widgets.Checkbox(value=True, description="Robust")
plot_pred_max_steps = widgets.IntSlider(
    min=1,
    max=predictions.dims["time"],
    value=predictions.dims["time"],
    description="Max steps")
plot_pred_samples = widgets.IntSlider(
    min=1,
    max=num_ensemble_members,
    value=num_ensemble_members,
    description="Samples")

widgets.VBox([
    plot_pred_variable,
    plot_pred_level,
    plot_pred_robust,
    plot_pred_max_steps,
    plot_pred_samples,
    widgets.Label(value="Run the next cell to plot the predictions. Rerunning this cell clears your selection.")
])

# @title Plot prediction samples and diffs

plot_size = 5
plot_max_steps = min(predictions.dims["time"], plot_pred_max_steps.value)

fig_title = plot_pred_variable.value
if "level" in predictions[plot_pred_variable.value].coords:
  fig_title += f" at {plot_pred_level.value} hPa"

for sample_idx in range(plot_pred_samples.value):
  data = {
      "Targets": scale(select(eval_targets, plot_pred_variable.value, plot_pred_level.value, plot_max_steps), robust=plot_pred_robust.value),
      "Predictions": scale(select(predictions.isel(sample=sample_idx), plot_pred_variable.value, plot_pred_level.value, plot_max_steps), robust=plot_pred_robust.value),
      "Diff": scale((select(eval_targets, plot_pred_variable.value, plot_pred_level.value, plot_max_steps) -
                          select(predictions.isel(sample=sample_idx), plot_pred_variable.value, plot_pred_level.value, plot_max_steps)),
                        robust=plot_pred_robust.value, center=0),
  }
  display.display(plot_data(data, fig_title + f", Sample {sample_idx}", plot_size, plot_pred_robust.value))

# @title Plot ensemble mean and CRPS

def crps(targets, predictions, bias_corrected = True):
  if predictions.sizes.get("sample", 1) < 2:
    raise ValueError(
        "predictions must have dim 'sample' with size at least 2.")
  sum_dims = ["sample", "sample2"]
  preds2 = predictions.rename({"sample": "sample2"})
  num_samps = predictions.sizes["sample"]
  num_samps2 = (num_samps - 1) if bias_corrected else num_samps
  mean_abs_diff = np.abs(
      predictions - preds2).sum(
          dim=sum_dims, skipna=False) / (num_samps * num_samps2)
  mean_abs_err = np.abs(targets - predictions).sum(dim="sample", skipna=False) / num_samps
  return mean_abs_err - 0.5 * mean_abs_diff


plot_size = 5
plot_max_steps = min(predictions.dims["time"], plot_pred_max_steps.value)

fig_title = plot_pred_variable.value
if "level" in predictions[plot_pred_variable.value].coords:
  fig_title += f" at {plot_pred_level.value} hPa"

data = {
    "Targets": scale(select(eval_targets, plot_pred_variable.value, plot_pred_level.value, plot_max_steps), robust=plot_pred_robust.value),
    "Ensemble Mean": scale(select(predictions.mean(dim=["sample"]), plot_pred_variable.value, plot_pred_level.value, plot_max_steps), robust=plot_pred_robust.value),
    "Ensemble CRPS": scale(crps((select(eval_targets, plot_pred_variable.value, plot_pred_level.value, plot_max_steps)),
                        select(predictions, plot_pred_variable.value, plot_pred_level.value, plot_max_steps)),
                      robust=plot_pred_robust.value, center=0),
}
display.display(plot_data(data, fig_title, plot_size, plot_pred_robust.value))

# 保存预测数据（2019年的预测结果）
predictions.to_netcdf("predictions_2019.nc")
print("Predictions saved to predictions_2019.nc")

# 保存真实数据（2019年的观测数据）
eval_targets.to_netcdf("observed_2019.nc")
print("Observed data saved to observed_2019.nc")

"""# Crop BC's 2019 prediction out"""

import xarray as xr
import numpy as np
import matplotlib.pyplot as plt

# ----------------------------
# 1. 从文件读取数据
# ----------------------------
# 如果已经保存了预测、真实和 diff 数据，则直接打开文件
# 否则，这里假设全局变量 predictions、eval_targets 已经生成。
# 例如：
# pred_ds = xr.open_dataset("predictions_2019.nc")
# obs_ds  = xr.open_dataset("observed_2019.nc")
# diff_ds = xr.open_dataset("diff_2019.nc")
# 然后：
# predictions = pred_ds["2m_temperature"]
# eval_targets  = obs_ds["2m_temperature"]
# diff          = diff_ds["2m_temperature"]

# ----------------------------
# 2. 选取不列颠哥伦比亚区域（BC）
# ----------------------------
# 假设经度采用 0～360 表示，不列颠哥伦比亚（BC）区域大约在：
#   纬度: 48°～60°，经度: -139° ~ -114°，转换到 0～360 表示为：
#   经度: 221°～246°  (因为 -139+360=221，-114+360=246)
lat_min, lat_max = 48, 60
lon_min, lon_max = 221, 246

# 选取真实数据：确保 eval_targets 为 DataArray
if isinstance(eval_targets, xr.Dataset):
    # 例如取 "2m_temperature" 变量
    observed_da = eval_targets["2m_temperature"]
else:
    observed_da = eval_targets

# 预测数据：如果 predictions 为 Dataset，则取 "2m_temperature" 变量
if isinstance(predictions, xr.Dataset):
    prediction_da = predictions["2m_temperature"]
else:
    prediction_da = predictions

# 如果全局 diff 数据变量存在（原代码计算的 diff = observed - prediction），可直接使用；
# 否则，我们这里不重新计算（假设 diff 变量在全局中已有）
if "diff" in globals() and isinstance(diff, xr.DataArray):
    diff_da = diff
else:
    diff_da = observed_da - prediction_da  # 仅用于可视化；但原代码中 diff 已经计算好

# 为便于观察，这里只选取第一个 batch 和第一个 time 步
obs_global   = observed_da.isel(batch=0, time=0)
pred_global  = prediction_da.isel(batch=0, time=0)
diff_global  = diff_da.isel(batch=0, time=0)

# 从全局数据中裁剪出 BC 区域
obs_bc   = obs_global.sel(lat=slice(lat_min, lat_max), lon=slice(lon_min, lon_max))
pred_bc  = pred_global.sel(lat=slice(lat_min, lat_max), lon=slice(lon_min, lon_max))
diff_bc  = diff_global.sel(lat=slice(lat_min, lat_max), lon=slice(lon_min, lon_max))

# ----------------------------
# 3. 如果预测数据包含 ensemble 样本，则循环绘图
# ----------------------------
# 检查预测数据是否含有 "sample" 维度
if "sample" in prediction_da.dims:
    num_samples = prediction_da.sizes["sample"]
else:
    num_samples = 1

for sample_idx in range(num_samples):
    if "sample" in prediction_da.dims:
        pred_field = prediction_da.isel(sample=sample_idx, batch=0, time=0).sel(lat=slice(lat_min, lat_max), lon=slice(lon_min, lon_max))
        diff_field = observed_da.isel(batch=0, time=0).sel(lat=slice(lat_min, lat_max), lon=slice(lon_min, lon_max)) - pred_field
    else:
        pred_field = pred_bc
        diff_field = diff_bc

    # 将 DataArray 转换为 NumPy 数组
    obs_array  = np.asarray(obs_bc.values)
    pred_array = np.asarray(pred_field.values)
    diff_array = np.asarray(diff_field.values)

    # 对 diff 进行对称归一化
    max_abs = np.nanmax(np.abs(diff_array))
    vmin_val, vmax_val = -max_abs, max_abs

    fig, axes = plt.subplots(1, 3, figsize=(15, 5))

    im0 = axes[0].imshow(obs_array, origin="lower", cmap="viridis")
    axes[0].set_title("Observed (BC)")
    plt.colorbar(im0, ax=axes[0])

    im1 = axes[1].imshow(pred_array, origin="lower", cmap="viridis")
    axes[1].set_title(f"Prediction Sample {sample_idx} (BC)")
    plt.colorbar(im1, ax=axes[1])

    im2 = axes[2].imshow(diff_array, origin="lower", cmap="RdBu_r", vmin=vmin_val, vmax=vmax_val)
    axes[2].set_title("Diff (Obs - Pred, BC)")
    plt.colorbar(im2, ax=axes[2])

    plt.suptitle(f"2m_temperature, Sample {sample_idx}")
    plt.tight_layout()
    plt.show()

"""Ensemble Prediction Plot with different temperature threshodls (overall, top 40%, top 4%)"""

import xarray as xr
import numpy as np
import matplotlib.pyplot as plt

#############################################
# Evaluation on 2019 Data for BC Region
#############################################

# 1. Load observed and prediction data from files.
# These files should have been saved previously.
obs_ds = xr.open_dataset("observed_2019.nc")
pred_ds = xr.open_dataset("predictions_2019.nc")

# Extract the 2m_temperature variable.
eval_targets = obs_ds["2m_temperature"]
predictions = pred_ds["2m_temperature"]

# 2. Compute ensemble mean for the predictions and select batch 0.
# (Assuming predictions have shape (sample, batch, time, lat, lon))
ens_pred_2019 = predictions.mean(dim="sample").isel(batch=0)  # shape: (time, lat, lon)
obs_2019 = eval_targets.isel(batch=0)                           # shape: (time, lat, lon)

# 3. Crop the data to the BC region.
# BC region: Latitude 48°–60°, Longitude 221°–246° (0–360 notation)
lat_min, lat_max = 48, 60
lon_min, lon_max = 221, 246

obs_2019_BC = obs_2019.sel(lat=slice(lat_min, lat_max), lon=slice(lon_min, lon_max))
pred_2019_BC = ens_pred_2019.sel(lat=slice(lat_min, lat_max), lon=slice(lon_min, lon_max))
diff_2019_BC = obs_2019_BC - pred_2019_BC

print("2019 BC observation shape:", obs_2019_BC.shape)  # Expected: (time, lat, lon)
print("2019 BC prediction shape:", pred_2019_BC.shape)    # Expected: (time, lat, lon)

# 4. Compute thresholds for high temperature regions.
# For Top 40% region: use the 60th percentile threshold.
# For Top 4% region: use the 96th percentile threshold.
threshold_40_2019 = np.nanquantile(obs_2019_BC.values, 0.6)
threshold_4_2019  = np.nanquantile(obs_2019_BC.values, 0.96)
print("2019 BC 60th percentile (Top 40%) threshold:", threshold_40_2019)
print("2019 BC 96th percentile (Top 4%) threshold:", threshold_4_2019)

# 5. Compute evaluation metrics (over all time steps) using the ensemble mean.
mae_full_2019  = np.nanmean(np.abs(obs_2019_BC - pred_2019_BC))
rmse_full_2019 = np.sqrt(np.nanmean((obs_2019_BC - pred_2019_BC)**2))
bias_full_2019 = np.nanmean(pred_2019_BC - obs_2019_BC)
print("\nEvaluation Metrics for BC Full Range (Ensemble Mean, 2019):")
print(f"MAE: {mae_full_2019:.2f}")
print(f"RMSE: {rmse_full_2019:.2f}")
print(f"Bias (Predicted - Observed): {bias_full_2019:.2f}")

mask_40_2019 = obs_2019_BC >= threshold_40_2019
obs_40_2019 = obs_2019_BC.where(mask_40_2019)
pred_40_2019 = pred_2019_BC.where(mask_40_2019)
mae_40_2019  = np.nanmean(np.abs(obs_40_2019 - pred_40_2019))
rmse_40_2019 = np.sqrt(np.nanmean((obs_40_2019 - pred_40_2019)**2))
bias_40_2019 = np.nanmean(pred_40_2019 - obs_40_2019)
print("\nEvaluation Metrics for BC Top 40% (Ensemble Mean, 2019):")
print(f"Threshold (60th percentile): {threshold_40_2019:.2f}")
print(f"MAE: {mae_40_2019:.2f}")
print(f"RMSE: {rmse_40_2019:.2f}")
print(f"Bias (Predicted - Observed): {bias_40_2019:.2f}")

mask_4_2019 = obs_2019_BC >= threshold_4_2019
obs_4_2019 = obs_2019_BC.where(mask_4_2019)
pred_4_2019 = pred_2019_BC.where(mask_4_2019)
mae_4_2019  = np.nanmean(np.abs(obs_4_2019 - pred_4_2019))
rmse_4_2019 = np.sqrt(np.nanmean((obs_4_2019 - pred_4_2019)**2))
bias_4_2019 = np.nanmean(pred_4_2019 - obs_4_2019)
print("\nEvaluation Metrics for BC Top 4% (Ensemble Mean, 2019):")
print(f"Threshold (96th percentile): {threshold_4_2019:.2f}")
print(f"MAE: {mae_4_2019:.2f}")
print(f"RMSE: {rmse_4_2019:.2f}")
print(f"Bias (Predicted - Observed): {bias_4_2019:.2f}")

# 6. Plot representative time step figures for 2019.
# We'll use the first time step (time=0) to create 2D plots.

# Full Region Plot
obs_full_plot = obs_2019_BC.isel(time=0)
pred_full_plot = pred_2019_BC.isel(time=0)
diff_full_plot = obs_full_plot - pred_full_plot

obs_full_arr = np.asarray(obs_full_plot.values)
pred_full_arr = np.asarray(pred_full_plot.values)
diff_full_arr = np.asarray(diff_full_plot.values)
max_abs_full = np.nanmax(np.abs(diff_full_arr))
vmin_full, vmax_full = -max_abs_full, max_abs_full

fig_full, axes_full = plt.subplots(1, 3, figsize=(15,5))
im0 = axes_full[0].imshow(obs_full_arr, origin="lower", cmap="viridis")
axes_full[0].set_title("Observed (BC, Full, 2019)")
plt.colorbar(im0, ax=axes_full[0])
im1 = axes_full[1].imshow(pred_full_arr, origin="lower", cmap="viridis")
axes_full[1].set_title("Predicted (Ensemble Mean, BC, Full, 2019)")
plt.colorbar(im1, ax=axes_full[1])
im2 = axes_full[2].imshow(diff_full_arr, origin="lower", cmap="RdBu_r", vmin=vmin_full, vmax=vmax_full)
axes_full[2].set_title("Difference (Obs - Pred, Full, 2019)")
plt.colorbar(im2, ax=axes_full[2])
plt.suptitle("BC Full Range 2m Temperature (2019)")
plt.tight_layout()
plt.show()

# Top 40% Plot
obs_40_plot = obs_40_2019.isel(time=0)
pred_40_plot = pred_40_2019.isel(time=0)
diff_40_plot = obs_40_plot - pred_40_plot

obs_40_arr = np.asarray(obs_40_plot.values)
pred_40_arr = np.asarray(pred_40_plot.values)
diff_40_arr = np.asarray(diff_40_plot.values)
max_abs_40 = np.nanmax(np.abs(diff_40_arr))
vmin_40, vmax_40 = -max_abs_40, max_abs_40

fig_40, axes_40 = plt.subplots(1, 3, figsize=(15,5))
im0 = axes_40[0].imshow(obs_40_arr, origin="lower", cmap="viridis")
axes_40[0].set_title("Observed (BC, Top 40%, 2019)")
plt.colorbar(im0, ax=axes_40[0])
im1 = axes_40[1].imshow(pred_40_arr, origin="lower", cmap="viridis")
axes_40[1].set_title("Predicted (Ensemble Mean, BC, Top 40%, 2019)")
plt.colorbar(im1, ax=axes_40[1])
im2 = axes_40[2].imshow(diff_40_arr, origin="lower", cmap="RdBu_r", vmin=vmin_40, vmax=vmax_40)
axes_40[2].set_title("Difference (Obs - Pred, Top 40%, 2019)")
plt.colorbar(im2, ax=axes_40[2])
plt.suptitle("BC Top 40% 2m Temperature (2019)\nThreshold (60th percentile) = {:.2f} K".format(threshold_40_2019))
plt.tight_layout()
plt.show()

# Top 4% Plot
obs_4_plot = obs_4_2019.isel(time=0)
pred_4_plot = pred_4_2019.isel(time=0)
diff_4_plot = obs_4_plot - pred_4_plot

obs_4_arr = np.asarray(obs_4_plot.values)
pred_4_arr = np.asarray(pred_4_plot.values)
diff_4_arr = np.asarray(diff_4_plot.values)
max_abs_4 = np.nanmax(np.abs(diff_4_arr))
vmin_4, vmax_4 = -max_abs_4, max_abs_4

fig_4, axes_4 = plt.subplots(1, 3, figsize=(15,5))
im0 = axes_4[0].imshow(obs_4_arr, origin="lower", cmap="viridis")
axes_4[0].set_title("Observed (BC, Top 4%, 2019)")
plt.colorbar(im0, ax=axes_4[0])
im1 = axes_4[1].imshow(pred_4_arr, origin="lower", cmap="viridis")
axes_4[1].set_title("Predicted (Ensemble Mean, BC, Top 4%, 2019)")
plt.colorbar(im1, ax=axes_4[1])
im2 = axes_4[2].imshow(diff_4_arr, origin="lower", cmap="RdBu_r", vmin=vmin_4, vmax=vmax_4)
axes_4[2].set_title("Difference (Obs - Pred, Top 4%, 2019)")
plt.colorbar(im2, ax=axes_4[2])
plt.suptitle("BC Top 4% 2m Temperature (2019)\nThreshold (96th percentile) = {:.2f} K".format(threshold_4_2019))
plt.tight_layout()
plt.show()

"""# Summary for 2019 BC Evaluation:

**1. Full Region:**

MAE: 0.72 K
RMSE: 0.98 K
Bias: +0.20 K

**2. Top 40% Region (Threshold = 281.83 K):**

MAE: 0.78 K
RMSE: 1.01 K
Bias: ≈ 0 K

**3. Top 4% Region (Threshold = 286.85 K):**

MAE: 1.06 K
RMSE: 1.30 K
Bias: -0.68 K

**Conclusion**:

For 2019, GenCast (trained up to 2018) shows very good performance over the full BC region with errors below 1 K. In high-temperature regions, errors increase slightly, and for the most extreme (top 4%) areas, the model tends to underpredict (bias of -0.68 K). These results suggest that the model is more accurate overall, but extreme events are more challenging.
"""

# circle out the hot areas and compare visually

import xarray as xr
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# ========== Step 1: Load 2019 Observation and Prediction Data ==========
# Load observed data and predictions from saved files.
obs_2019 = xr.open_dataset("observed_2019.nc")["2m_temperature"].isel(batch=0)  # dims: time, lat, lon
pred_2019 = xr.open_dataset("predictions_2019.nc")["2m_temperature"]
# Compute the ensemble mean (average over 'sample') and select batch 0.
ens_pred_2019 = pred_2019.mean(dim="sample").isel(batch=0)  # dims: time, lat, lon

# ========== Step 2: Crop to BC Region ==========
# Define BC region: Latitude 48–60°, Longitude 221–246° (assuming longitude is in 0–360)
lat_min, lat_max = 48, 60
lon_min, lon_max = 221, 246

obs_2019_BC = obs_2019.sel(lat=slice(lat_min, lat_max), lon=slice(lon_min, lon_max))
pred_2019_BC = ens_pred_2019.sel(lat=slice(lat_min, lat_max), lon=slice(lon_min, lon_max))

print("2019 BC observation shape:", obs_2019_BC.shape)
print("2019 BC prediction shape:", pred_2019_BC.shape)

# ========== Step 3: Compute the 96th Percentile Threshold ==========
threshold_96_2019 = np.nanquantile(obs_2019_BC.values, 0.96)
print("2019 BC 96th percentile threshold:", threshold_96_2019)

# ========== Step 4: Select a Representative Time Step ==========
# Select the first time step as the representative moment.
rep_time_2019 = obs_2019_BC.time.values[0]
# Convert the representative time (which may be timedelta64) to string.
rep_time_str = str(rep_time_2019)
print("Representative time:", rep_time_str)

obs_rep_2019 = obs_2019_BC.isel(time=0)
pred_rep_2019 = pred_2019_BC.isel(time=0)
diff_rep_2019 = obs_rep_2019 - pred_rep_2019

# Compute the mask for the hottest areas (temperature > 96th percentile).
mask_rep_2019 = obs_rep_2019 > threshold_96_2019

mask_float = mask_rep_2019.astype(float)
axes[0].contour(mask_float, levels=[0.5], colors='red', linewidths=1)
print("Number of grid points above threshold:", np.sum(mask_rep_2019.values))


# Determine a symmetric range for the error plot.
max_abs_rep_2019 = np.nanmax(np.abs(diff_rep_2019.values))
vmin_rep_2019, vmax_rep_2019 = -max_abs_rep_2019, max_abs_rep_2019

# ========== Step 5: Plot the Results ==========
fig, axes = plt.subplots(1, 3, figsize=(18, 6))

# Panel 1: Observed field with red contour outlining grid points above the threshold.
im0 = axes[0].imshow(obs_rep_2019, origin="lower", cmap="viridis")
axes[0].contour(mask_rep_2019, colors='red', linewidths=1)
axes[0].set_title(f"Observation (BC) @ {rep_time_str}\nRed contour: >96th percentile")
plt.colorbar(im0, ax=axes[0])

# Panel 2: Ensemble Mean Prediction.
im1 = axes[1].imshow(pred_rep_2019, origin="lower", cmap="viridis")
axes[1].set_title(f"Prediction (Ensemble Mean, BC) @ {rep_time_str}")
plt.colorbar(im1, ax=axes[1])

# Panel 3: Difference (Observation - Prediction).
im2 = axes[2].imshow(diff_rep_2019, origin="lower", cmap="RdBu_r", vmin=vmin_rep_2019, vmax=vmax_rep_2019)
axes[2].set_title("Error (Observation - Prediction)")
plt.colorbar(im2, ax=axes[2])

plt.suptitle("GenCast Evaluation for Top 4% 2m Temperature (BC, 2019)\nThreshold = {:.2f} K (96th percentile)".format(threshold_96_2019))
plt.tight_layout()
plt.show()

"""# Use 2022 data to predict"""

!pip install --upgrade netCDF4

import netCDF4
import xarray as xr

local_path = "/content/gencast_dataset_source-hres_date-2022-03-29_res-0.25_levels-13_steps-01.nc"
example_batch = xr.load_dataset(local_path, engine="netcdf4").compute()
print("Loaded local dataset with dims:", example_batch.dims)

"""## Print the info of 2022 New Data"""

import xarray as xr
import numpy as np

# 正确的2022数据路径
file_path_2022 = "/content/gencast_dataset_source-hres_date-2022-03-29_res-0.25_levels-13_steps-01.nc"
example_batch = xr.open_dataset(file_path_2022)

print("=== Full Dataset Information (2022) ===")
print(example_batch)

# 检查是否包含变量 "2m_temperature"
if "2m_temperature" in example_batch.data_vars:
    obs_data = example_batch["2m_temperature"]
    print("\n=== Observed Data (2m_temperature) ===")
    print(obs_data)

    print("\n=== Dimensions and Sizes ===")
    for dim, size in obs_data.sizes.items():
        print(f"{dim}: {size}")

    print("\n=== Coordinate Details ===")
    for coord in obs_data.coords:
        print(f"{coord}: {obs_data.coords[coord].values}")

    if "time" in obs_data.coords:
        time_values = obs_data.time.values
        print("\n=== Time Coordinate ===")
        print(time_values)

        time_diffs = np.diff(time_values)
        print("\n=== Raw Time Differences between Consecutive Points ===")
        print(time_diffs)

        time_diffs_hours = (time_diffs / np.timedelta64(1, 'h')).astype(float)
        print("\nTime Differences (in hours):")
        print(time_diffs_hours)
else:
    print("Variable '2m_temperature' is not found in the dataset.")

"""Use 2022's data, downsample, and predict"""

#########################################
# Test GenCast accuracy for the hottest 40% region on 2022 data
#########################################

import xarray
import xarray as xr
import numpy as np
import jax
import xarray  # for xarray.combine_by_coords
# 注意：确保已经导入 data_utils, rollout, run_forward_pmap, crps, dataclasses, task_config 等

# 1. 加载2022年3月的 HRES 数据（正确的文件路径）
file_path_2022 = "/content/gencast_dataset_source-hres_date-2022-03-29_res-0.25_levels-13_steps-01.nc"
new_example_batch = xr.open_dataset(file_path_2022).compute()
print("Loaded 2022 dataset with dims:", new_example_batch.dims)

# 2. 修改数据结构：将0.25°数据下采样到1°（迷你模型所需）
# 检测经纬度坐标名称
if "latitude" in new_example_batch.coords and "longitude" in new_example_batch.coords:
    lat_coord, lon_coord = "latitude", "longitude"
elif "lat" in new_example_batch.coords and "lon" in new_example_batch.coords:
    lat_coord, lon_coord = "lat", "lon"
else:
    raise ValueError("无法识别新数据中的经纬度坐标。")

# 下采样：简单每隔4个点取一个
new_example_batch_downscaled = new_example_batch.isel({lat_coord: slice(0, None, 4),
                                                        lon_coord: slice(0, None, 4)})
print("Downscaled 2022 dataset dims:", new_example_batch_downscaled.dims)

# 3. 从下采样后的数据中提取输入、目标和强迫场
train_inputs_new, train_targets_new, train_forcings_new = data_utils.extract_inputs_targets_forcings(
    new_example_batch_downscaled,
    target_lead_times=slice("12h", "12h"),  # 只使用1AR训练
    **dataclasses.asdict(task_config)
)
eval_inputs_new, eval_targets_new, eval_forcings_new = data_utils.extract_inputs_targets_forcings(
    new_example_batch_downscaled,
    target_lead_times=slice("12h", f"{(new_example_batch_downscaled.dims['time']-2)*12}h"),
    **dataclasses.asdict(task_config)
)

print("2022 New Data:")
print(" Train Inputs dims:", train_inputs_new.dims.mapping)
print(" Eval Inputs dims:", eval_inputs_new.dims.mapping)

# 4. 用模型对2022数据做预测（沿用原推断流程）
num_ensemble_members_new = 8  # 使用8成员集合
rng_new = jax.random.PRNGKey(42)
rngs_new = np.stack([jax.random.fold_in(rng_new, i) for i in range(num_ensemble_members_new)], axis=0)

chunks_new = []
for chunk in rollout.chunked_prediction_generator_multiple_runs(
    predictor_fn=run_forward_pmap,  # 使用已构建的pmapped预测函数
    rngs=rngs_new,
    inputs=eval_inputs_new,
    targets_template=eval_targets_new * np.nan,
    forcings=eval_forcings_new,
    num_steps_per_chunk=1,
    num_samples=num_ensemble_members_new,
    pmap_devices=jax.local_devices()
):
    chunks_new.append(chunk)
predictions_new = xarray.combine_by_coords(chunks_new)
print("Predictions on 2022 data dims:", predictions_new.dims)

# 5. 针对“最热40%”天气计算预测误差
# 以变量 "2m_temperature" 和大气层次500 hPa为例
variable_to_eval = "2m_temperature"
level_to_eval = 500  # 根据实际数据选择

# 利用原来的 select 函数提取目标和预测数据（确保 select 函数已定义）
eval_targets_var = select(eval_targets_new, variable_to_eval, level=level_to_eval)
predictions_var = select(predictions_new, variable_to_eval, level=level_to_eval)

# 计算“最热40%”的阈值：即目标温度的60百分位（上40%大于该阈值）
threshold = np.nanquantile(eval_targets_var, 0.6)
print(f"Hottest 40% threshold (60th percentile) for {variable_to_eval} at {level_to_eval} hPa: {threshold:.2f}")

# 构造掩码：只保留目标温度大于阈值的区域
hot_mask = eval_targets_var >= threshold

# 计算集合预测的均值
ensemble_mean_pred = predictions_var.mean(dim="sample")

# 计算最热40%区域的平均绝对误差（MAE）
mae_hot = np.abs(eval_targets_var.where(hot_mask) - ensemble_mean_pred.where(hot_mask)).mean().item()
print(f"Mean Absolute Error for hottest 40% {variable_to_eval} forecasts: {mae_hot:.2f}")

# 计算CRPS指标（使用原代码中的 crps 函数），仅对最热区域计算
crps_hot = crps(eval_targets_var.where(hot_mask), predictions_var.where(hot_mask))
crps_hot_mean = crps_hot.mean().item()
print(f"CRPS for hottest 40% {variable_to_eval} forecasts: {crps_hot_mean:.2f}")

"""Plot prediction for 2022 March 29 12PM's t2m"""

# “全区域温度”图
init_time = str(eval_targets_var.time.values[0])
plot_full_data = {
    "Targets (Full)": scale(eval_targets_var, robust=True),
    "Ensemble Mean (Full)": scale(ensemble_mean_pred, robust=True),
    "Diff (Full)": scale(eval_targets_var - ensemble_mean_pred, robust=True, center=0),
}
fig_title_full = f"{variable_to_eval} at {level_to_eval} hPa (All Temperatures)\nForecast init: {init_time}"
display.display(plot_data(plot_full_data, fig_title_full, plot_size=7, robust=True))

"""Plot the predictions for the highest 40% t2m"""

# “最热40%区域”图
# 获取预报起始时间（取目标场 time 坐标的第一个时间），直接转换为字符串
init_time = str(eval_targets_var.time.values[0])

plot_hot_data = {
    "Targets (Hot 40%)": scale(eval_targets_var.where(hot_mask), robust=True),
    "Ensemble Mean (Hot 40%)": scale(ensemble_mean_pred.where(hot_mask), robust=True),
    "Diff (Hot 40%)": scale((eval_targets_var.where(hot_mask) - ensemble_mean_pred.where(hot_mask)),
                             robust=True, center=0),
}
fig_title_hot = f"{variable_to_eval} at {level_to_eval} hPa (Hottest 40%)\nForecast init: {init_time}"
display.display(plot_data(plot_hot_data, fig_title_hot, plot_size=7, robust=True))

#diff was scaled for plotting. What is the real range?

diff_data = eval_targets_var - predictions_var
print("Diff min:", float(diff_data.min()), "max:", float(diff_data.max()))

"""Crop BC area out (Esemble prediction)"""

import numpy as np
import xarray as xr
import matplotlib.pyplot as plt
import pandas as pd

# ========== Step 1: 提取2022年观测和预测数据 ==========
# 从 example_batch 中提取2022年的观测数据（2m_temperature）
obs_2022 = example_batch["2m_temperature"].isel(batch=0)  # dims: time, lat, lon

# 对于预测数据，假设变量为 predictions_new（如果是 Dataset，则提取 "2m_temperature"）
if isinstance(predictions_new, xr.Dataset):
    pred_data_2022 = predictions_new["2m_temperature"]
else:
    pred_data_2022 = predictions_new
# 计算集合均值（对 sample 维度求平均），取 batch=0
ens_pred_2022 = pred_data_2022.mean(dim="sample").isel(batch=0)  # dims: time, lat, lon

# ========== Step 2: 裁剪 BC 区域 ==========
# 定义 BC 区域：纬度48～60，经度221～246（0～360表示）
lat_min, lat_max = 48, 60
lon_min, lon_max = 221, 246

obs_2022_BC = obs_2022.sel(lat=slice(lat_min, lat_max), lon=slice(lon_min, lon_max))
pred_2022_BC = ens_pred_2022.sel(lat=slice(lat_min, lat_max), lon=slice(lon_min, lon_max))

print("2022 BC observation shape:", obs_2022_BC.shape)
print("2022 BC prediction shape:", pred_2022_BC.shape)

# ========== Step 3: 计算 BC 区域的 96th 百分位温度阈值 ==========
threshold_96_2022 = np.nanquantile(obs_2022_BC.values, 0.96)
print("2022 BC 96th percentile threshold:", threshold_96_2022)

# ========== Step 4: 选取代表时刻 ==========
# 选择第一个时间步作为代表
rep_time_2022 = obs_2022_BC.time.values[0]
# rep_time_2022 可能为 timedelta64 类型，直接转换为字符串显示
rep_time_str = str(rep_time_2022)
print("Representative time:", rep_time_str)

obs_rep_2022 = obs_2022_BC.isel(time=0)
pred_rep_2022 = pred_2022_BC.isel(time=0)
diff_rep_2022 = obs_rep_2022 - pred_rep_2022

# 计算代表时刻中温度大于阈值的区域（高温区域）
mask_rep_2022 = obs_rep_2022 > threshold_96_2022

# 计算误差的色标范围（对称）
max_abs_rep_2022 = np.nanmax(np.abs(diff_rep_2022.values))
vmin_rep_2022, vmax_rep_2022 = -max_abs_rep_2022, max_abs_rep_2022

# ========== Step 5: 绘制图形 ==========
fig, axes = plt.subplots(1, 3, figsize=(18, 6))

# Panel 1: Observation with high temperature areas outlined in red
im0 = axes[0].imshow(obs_rep_2022, origin="lower", cmap="viridis")
axes[0].contour(mask_rep_2022, colors='red', linewidths=1)
axes[0].set_title(f"Observation (BC) @ {rep_time_str}\nRed contour: >96th percentile")
plt.colorbar(im0, ax=axes[0])

# Panel 2: Ensemble Mean Prediction
im1 = axes[1].imshow(pred_rep_2022, origin="lower", cmap="viridis")
axes[1].set_title(f"Prediction (Ensemble Mean, BC) @ {rep_time_str}")
plt.colorbar(im1, ax=axes[1])

# Panel 3: Error (Observation - Prediction)
im2 = axes[2].imshow(diff_rep_2022, origin="lower", cmap="RdBu_r", vmin=vmin_rep_2022, vmax=vmax_rep_2022)
axes[2].set_title("Error (Observation - Prediction)")
plt.colorbar(im2, ax=axes[2])

plt.suptitle("GenCast Evaluation for High Temperature Region (BC, 2022)\nThreshold = {:.2f} K (96th percentile)".format(threshold_96_2022))
plt.tight_layout()
plt.show()

"""## Evaluate how GenCast performs in predicting the high t2ms in British Columbia"""

import xarray as xr
import numpy as np
import matplotlib.pyplot as plt

# ----------------------------
# 1. Read the data (if already saved, load from file)
# ----------------------------
# For 2022 predictions, assume the following global variables exist:
# predictions_new: predicted 2m_temperature (either a DataArray or Dataset)
# eval_targets_new: observed 2m_temperature (either a DataArray or Dataset)
#
# If saved as files, you might do:
#   pred_ds_2022 = xr.open_dataset("predictions_2022.nc")
#   obs_ds_2022  = xr.open_dataset("observed_2022.nc")
#   diff_ds_2022 = xr.open_dataset("diff_2022.nc")
# then set:
# predictions_new = pred_ds_2022["2m_temperature"]
# eval_targets_new  = obs_ds_2022["2m_temperature"]
# diff_2022         = diff_ds_2022["2m_temperature"]

# ----------------------------
# 2. Select the British Columbia (BC) region
# ----------------------------
# Define BC boundaries.
# For longitude expressed as 0–360, the BC region (approx.) is:
#   Latitude: 48° to 60°
#   Longitude: -139° to -114° => Converted to 0–360: 221° to 246°
lat_min, lat_max = 48, 60
lon_min, lon_max = 221, 246

# Select observed data (ensure eval_targets_new is a DataArray)
if isinstance(eval_targets_new, xr.Dataset):
    observed_da_2022 = eval_targets_new["2m_temperature"]
else:
    observed_da_2022 = eval_targets_new

# Select prediction data (if predictions_new is a Dataset, extract variable)
if isinstance(predictions_new, xr.Dataset):
    prediction_da_2022 = predictions_new["2m_temperature"]
else:
    prediction_da_2022 = predictions_new

# Use diff data if available; otherwise, compute it.
if "diff_2022" in globals() and isinstance(diff_2022, xr.DataArray):
    diff_da_2022 = diff_2022
else:
    diff_da_2022 = observed_da_2022 - prediction_da_2022  # for visualization

# For easier viewing, select the first batch and first time step.
obs_global_2022  = observed_da_2022.isel(batch=0, time=0)
pred_global_2022 = prediction_da_2022.isel(batch=0, time=0)
diff_global_2022 = diff_da_2022.isel(batch=0, time=0)

# Crop out the BC area from the global data.
obs_bc_2022   = obs_global_2022.sel(lat=slice(lat_min, lat_max), lon=slice(lon_min, lon_max))
pred_bc_2022  = pred_global_2022.sel(lat=slice(lat_min, lat_max), lon=slice(lon_min, lon_max))
diff_bc_2022  = diff_global_2022.sel(lat=slice(lat_min, lat_max), lon=slice(lon_min, lon_max))

# ----------------------------
# 3. Plot each ensemble sample (if ensemble dimension exists)
# ----------------------------
if "sample" in prediction_da_2022.dims:
    num_samples = prediction_da_2022.sizes["sample"]
else:
    num_samples = 1

for sample_idx in range(num_samples):
    if "sample" in prediction_da_2022.dims:
        # For each ensemble sample, crop the BC region
        pred_field_2022 = prediction_da_2022.isel(sample=sample_idx, batch=0, time=0)\
                            .sel(lat=slice(lat_min, lat_max), lon=slice(lon_min, lon_max))
        diff_field_2022 = observed_da_2022.isel(batch=0, time=0)\
                            .sel(lat=slice(lat_min, lat_max), lon=slice(lon_min, lon_max)) - pred_field_2022
    else:
        pred_field_2022 = pred_bc_2022
        diff_field_2022 = diff_bc_2022

    # Convert DataArrays to NumPy arrays for plotting.
    obs_array_2022  = np.asarray(obs_bc_2022.values)
    pred_array_2022 = np.asarray(pred_field_2022.values)
    diff_array_2022 = np.asarray(diff_field_2022.values)

    # Set a symmetric normalization range for the difference plot.
    max_abs = np.nanmax(np.abs(diff_array_2022))
    vmin_val, vmax_val = -max_abs, max_abs

    fig, axes = plt.subplots(1, 3, figsize=(15, 5))

    im0 = axes[0].imshow(obs_array_2022, origin="lower", cmap="viridis")
    axes[0].set_title("Observed (BC, 2022)")
    plt.colorbar(im0, ax=axes[0])

    im1 = axes[1].imshow(pred_array_2022, origin="lower", cmap="viridis")
    axes[1].set_title(f"Prediction Sample {sample_idx} (BC, 2022)")
    plt.colorbar(im1, ax=axes[1])

    im2 = axes[2].imshow(diff_array_2022, origin="lower", cmap="RdBu_r", vmin=vmin_val, vmax=vmax_val)
    axes[2].set_title("Diff (Obs - Pred, BC, 2022)")
    plt.colorbar(im2, ax=axes[2])

    plt.suptitle(f"2m_temperature, Sample {sample_idx} (2022)")
    plt.tight_layout()
    plt.show()

# ----------------------------
# Ensemble Mean Plot for BC (2022)
# ----------------------------

# If the predictions include an ensemble (i.e. a "sample" dimension), compute the ensemble mean.
if "sample" in prediction_da_2022.dims:
    ensemble_mean = prediction_da_2022.mean(dim="sample")
else:
    ensemble_mean = prediction_da_2022

# Select the first batch and time step from the ensemble mean.
ensemble_mean_global = ensemble_mean.isel(batch=0, time=0)
# Crop the BC region (using the same lat/lon boundaries defined earlier)
ensemble_mean_bc = ensemble_mean_global.sel(lat=slice(lat_min, lat_max), lon=slice(lon_min, lon_max))

# For comparison, crop the observed data (already done for 2022)
obs_global_2022 = observed_da_2022.isel(batch=0, time=0)
obs_bc_2022 = obs_global_2022.sel(lat=slice(lat_min, lat_max), lon=slice(lon_min, lon_max))

# Calculate the difference between the observed field and the ensemble mean.
diff_ensemble_bc = obs_bc_2022 - ensemble_mean_bc

# Convert DataArrays to NumPy arrays for plotting.
obs_array = np.asarray(obs_bc_2022.values)
ens_array = np.asarray(ensemble_mean_bc.values)
diff_array = np.asarray(diff_ensemble_bc.values)

# Set up symmetric normalization for the diff plot.
max_abs = np.nanmax(np.abs(diff_array))
vmin_val, vmax_val = -max_abs, max_abs

# Plot the Observed field, Ensemble Mean, and the Difference.
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

im0 = axes[0].imshow(obs_array, origin="lower", cmap="viridis")
axes[0].set_title("Observed (BC, 2022)")
plt.colorbar(im0, ax=axes[0])

im1 = axes[1].imshow(ens_array, origin="lower", cmap="viridis")
axes[1].set_title("Ensemble Mean (BC, 2022)")
plt.colorbar(im1, ax=axes[1])

im2 = axes[2].imshow(diff_array, origin="lower", cmap="RdBu_r", vmin=vmin_val, vmax=vmax_val)
axes[2].set_title("Diff (Obs - EnsMean, BC, 2022)")
plt.colorbar(im2, ax=axes[2])

plt.suptitle("2m_temperature Ensemble Mean (2022)")
plt.tight_layout()
plt.show()

import numpy as np
import matplotlib.pyplot as plt

# ----------------------------
# Setup BC boundaries
# ----------------------------
lat_min, lat_max = 48, 60
lon_min, lon_max = 221, 246

# ============================
# 1. FULL REGION EVALUATION & PLOTTING
# ============================
# (Assuming obs_bc_2022 and ensemble_mean_bc are already BC-cropped DataArrays for batch=0, time=0)
obs_full = obs_bc_2022
ens_full = ensemble_mean_bc

# Compute evaluation metrics
mae_full   = np.nanmean(np.abs(obs_full - ens_full))
rmse_full  = np.sqrt(np.nanmean((obs_full - ens_full)**2))
bias_full  = np.nanmean(ens_full - obs_full)

# Create plots for Full Region
obs_full_arr  = np.asarray(obs_full.values)
ens_full_arr  = np.asarray(ens_full.values)
diff_full_arr = np.asarray(obs_full.values - ens_full.values)
max_abs_full  = np.nanmax(np.abs(diff_full_arr))
vmin_full, vmax_full = -max_abs_full, max_abs_full

fig1, axes1 = plt.subplots(1, 3, figsize=(15,5))
im0 = axes1[0].imshow(obs_full_arr, origin="lower", cmap="viridis")
axes1[0].set_title("Observed (BC, Full)")
plt.colorbar(im0, ax=axes1[0])
im1 = axes1[1].imshow(ens_full_arr, origin="lower", cmap="viridis")
axes1[1].set_title("Predicted (Ensemble Mean, BC, Full)")
plt.colorbar(im1, ax=axes1[1])
im2 = axes1[2].imshow(diff_full_arr, origin="lower", cmap="RdBu_r", vmin=vmin_full, vmax=vmax_full)
axes1[2].set_title("Difference (Obs - Pred, Full)")
plt.colorbar(im2, ax=axes1[2])
plt.suptitle("BC Full Range 2m Temperature (2022)")
plt.tight_layout()
plt.show()

print("Evaluation Metrics for BC Full Range (Ensemble Mean, 2022):")
print(f"MAE: {mae_full:.2f}")
print(f"RMSE: {rmse_full:.2f}")
print(f"Bias (Predicted - Observed): {bias_full:.2f}")

# ============================
# 2. TOP 40% REGION EVALUATION & PLOTTING
# ============================
# Define threshold for top 40%: 60th percentile
threshold_40 = np.nanquantile(obs_full.values, 0.6)
mask_40      = obs_full >= threshold_40
obs_40       = obs_full.where(mask_40)
ens_40       = ens_full.where(mask_40)

mae_40   = np.nanmean(np.abs(obs_40 - ens_40))
rmse_40  = np.sqrt(np.nanmean((obs_40 - ens_40)**2))
bias_40  = np.nanmean(ens_40 - obs_40)

obs_40_arr  = np.asarray(obs_40.values)
ens_40_arr  = np.asarray(ens_40.values)
diff_40_arr = np.asarray(obs_40.values - ens_40.values)
max_abs_40  = np.nanmax(np.abs(diff_40_arr))
vmin_40, vmax_40 = -max_abs_40, max_abs_40

fig2, axes2 = plt.subplots(1, 3, figsize=(15,5))
im0 = axes2[0].imshow(obs_40_arr, origin="lower", cmap="viridis")
axes2[0].set_title("Observed (BC, Top 40%)")
plt.colorbar(im0, ax=axes2[0])
im1 = axes2[1].imshow(ens_40_arr, origin="lower", cmap="viridis")
axes2[1].set_title("Predicted (Ensemble Mean, BC, Top 40%)")
plt.colorbar(im1, ax=axes2[1])
im2 = axes2[2].imshow(diff_40_arr, origin="lower", cmap="RdBu_r", vmin=vmin_40, vmax=vmax_40)
axes2[2].set_title("Difference (Obs - Pred, Top 40%)")
plt.colorbar(im2, ax=axes2[2])
plt.suptitle("BC Top 40% 2m Temperature (2022)\nThreshold (60th percentile) = {:.2f} K".format(threshold_40))
plt.tight_layout()
plt.show()

print("\nEvaluation Metrics for BC Top 40% (Ensemble Mean, 2022):")
print(f"High Temperature Threshold (60th percentile): {threshold_40:.2f}")
print(f"MAE: {mae_40:.2f}")
print(f"RMSE: {rmse_40:.2f}")
print(f"Bias (Predicted - Observed): {bias_40:.2f}")

# ============================
# 3. TOP 4% REGION EVALUATION & PLOTTING
# ============================
# Define threshold for top 4%: 96th percentile
threshold_4 = np.nanquantile(obs_full.values, 0.96)
mask_4      = obs_full >= threshold_4
obs_4       = obs_full.where(mask_4)
ens_4       = ens_full.where(mask_4)

mae_4   = np.nanmean(np.abs(obs_4 - ens_4))
rmse_4  = np.sqrt(np.nanmean((obs_4 - ens_4)**2))
bias_4  = np.nanmean(ens_4 - obs_4)

obs_4_arr  = np.asarray(obs_4.values)
ens_4_arr  = np.asarray(ens_4.values)
diff_4_arr = np.asarray(obs_4.values - ens_4.values)
max_abs_4  = np.nanmax(np.abs(diff_4_arr))
vmin_4, vmax_4 = -max_abs_4, max_abs_4

fig3, axes3 = plt.subplots(1, 3, figsize=(15,5))
im0 = axes3[0].imshow(obs_4_arr, origin="lower", cmap="viridis")
axes3[0].set_title("Observed (BC, Top 4%)")
plt.colorbar(im0, ax=axes3[0])
im1 = axes3[1].imshow(ens_4_arr, origin="lower", cmap="viridis")
axes3[1].set_title("Predicted (Ensemble Mean, BC, Top 4%)")
plt.colorbar(im1, ax=axes3[1])
im2 = axes3[2].imshow(diff_4_arr, origin="lower", cmap="RdBu_r", vmin=vmin_4, vmax=vmax_4)
axes3[2].set_title("Difference (Obs - Pred, Top 4%)")
plt.colorbar(im2, ax=axes3[2])
plt.suptitle("BC Top 4% 2m Temperature (2022)\nThreshold (96th percentile) = {:.2f} K".format(threshold_4))
plt.tight_layout()
plt.show()

print("\nEvaluation Metrics for BC Top 4% (Ensemble Mean, 2022):")
print(f"Top 4% Temperature Threshold (96th percentile): {threshold_4:.2f}")
print(f"MAE: {mae_4:.2f}")
print(f"RMSE: {rmse_4:.2f}")
print(f"Bias (Predicted - Observed): {bias_4:.2f}")

# ============================
# 4. Individual Ensemble Samples Evaluation
# ============================
if "sample" in prediction_da_2022.dims:
    num_samples = prediction_da_2022.sizes["sample"]
    print("\nEvaluation Metrics for Individual Ensemble Samples:")
    for s in range(num_samples):
        # Get the sample field cropped to BC
        sample_field = prediction_da_2022.isel(sample=s, batch=0, time=0).sel(lat=slice(lat_min, lat_max), lon=slice(lon_min, lon_max))

        # For full region:
        sample_full = sample_field
        mae_sample_full  = np.nanmean(np.abs(obs_full - sample_full))
        rmse_sample_full = np.sqrt(np.nanmean((obs_full - sample_full)**2))
        bias_sample_full = np.nanmean(sample_full - obs_full)

        # For top 40%:
        sample_40 = sample_field.where(mask_40)
        mae_sample_40  = np.nanmean(np.abs(obs_40 - sample_40))
        rmse_sample_40 = np.sqrt(np.nanmean((obs_40 - sample_40)**2))
        bias_sample_40 = np.nanmean(sample_40 - obs_40)

        # For top 4%:
        sample_4 = sample_field.where(mask_4)
        mae_sample_4  = np.nanmean(np.abs(obs_4 - sample_4))
        rmse_sample_4 = np.sqrt(np.nanmean((obs_4 - sample_4)**2))
        bias_sample_4 = np.nanmean(sample_4 - obs_4)

        print(f"\nEnsemble Sample {s}:")
        print("  Full Region - MAE: {:.2f}, RMSE: {:.2f}, Bias: {:.2f}".format(mae_sample_full, rmse_sample_full, bias_sample_full))
        print("  Top 40%     - MAE: {:.2f}, RMSE: {:.2f}, Bias: {:.2f}".format(mae_sample_40, rmse_sample_40, bias_sample_40))
        print("  Top 4%      - MAE: {:.2f}, RMSE: {:.2f}, Bias: {:.2f}".format(mae_sample_4, rmse_sample_4, bias_sample_4))

import xarray as xr
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# ============================
# Step 1: Load 2022 Dataset and Print Information
# ============================
file_path_2022 = "/content/gencast_dataset_source-hres_date-2022-03-29_res-0.25_levels-13_steps-01.nc"
example_batch = xr.open_dataset(file_path_2022)

print("=== Full Dataset Information (2022) ===")
print(example_batch)

# Check if the dataset contains the variable "2m_temperature"
if "2m_temperature" in example_batch.data_vars:
    obs_data = example_batch["2m_temperature"]
    print("\n=== Observed Data (2m_temperature) ===")
    print(obs_data)

    print("\n=== Dimensions and Sizes ===")
    for dim, size in obs_data.sizes.items():
        print(f"{dim}: {size}")

    print("\n=== Coordinate Details ===")
    for coord in obs_data.coords:
        print(f"{coord}: {obs_data.coords[coord].values}")

    # Print time coordinate information (if available)
    if "time" in obs_data.coords:
        time_values = obs_data.time.values
        print("\n=== Time Coordinate ===")
        print(time_values)

        # Calculate and print time differences between consecutive time points
        time_diffs = np.diff(time_values)
        print("\n=== Raw Time Differences between Consecutive Points ===")
        print(time_diffs)

        # Convert time differences to hours (if time is datetime64)
        time_diffs_hours = (time_diffs / np.timedelta64(1, 'h')).astype(float)
        print("\nTime Differences (in hours):")
        print(time_diffs_hours)
else:
    print("Variable '2m_temperature' is not found in the dataset.")

# ============================
# Step 2: Extract Observed and Prediction Data (2022)
# ============================
# Observed data from the 2022 dataset (dimensions: time, lat, lon; using batch=0)
obs_2022 = obs_data.isel(batch=0)

# For predictions, assume the prediction data is stored in 'predictions_new'.
# If predictions_new is a Dataset, extract the variable "2m_temperature".
if isinstance(predictions_new, xr.Dataset):
    pred_data_2022 = predictions_new["2m_temperature"]
else:
    pred_data_2022 = predictions_new

# Compute ensemble mean (averaging over the 'sample' dimension) and select batch=0.
ens_pred_2022 = pred_data_2022.mean(dim="sample").isel(batch=0)

# ============================
# Step 3: Crop to BC Region
# ============================
# Define BC region: latitude 48 to 60, longitude 221 to 246 (assuming lon is 0–360)
lat_min, lat_max = 48, 60
lon_min, lon_max = 221, 246

obs_2022_BC = obs_2022.sel(lat=slice(lat_min, lat_max), lon=slice(lon_min, lon_max))
pred_2022_BC = ens_pred_2022.sel(lat=slice(lat_min, lat_max), lon=slice(lon_min, lon_max))

print("\n2022 BC Observation shape:", obs_2022_BC.shape)
print("2022 BC Prediction shape:", pred_2022_BC.shape)

# ============================
# Step 4: Compute the 96th Percentile Threshold for BC
# ============================
threshold_96_2022 = np.nanquantile(obs_2022_BC.values, 0.96)
print("\n2022 BC 96th Percentile Threshold:", threshold_96_2022)

# ============================
# Step 5: Select a Representative Time Step and Prepare Data for Plotting
# ============================
# Choose the first time step as the representative moment.
rep_time_2022 = obs_2022_BC.time.values[0]
# Since rep_time_2022 may be of type timedelta64, convert it to a string.
rep_time_str = str(rep_time_2022)
print("Representative time:", rep_time_str)

obs_rep_2022 = obs_2022_BC.isel(time=0)
pred_rep_2022 = pred_2022_BC.isel(time=0)
diff_rep_2022 = obs_rep_2022 - pred_rep_2022

# Compute high temperature mask at the representative time step:
mask_rep_2022 = obs_rep_2022 > threshold_96_2022

# Determine symmetric range for error plot:
max_abs_rep_2022 = np.nanmax(np.abs(diff_rep_2022.values))
vmin_rep_2022, vmax_rep_2022 = -max_abs_rep_2022, max_abs_rep_2022

# ============================
# Step 6: Plot the Results in English
# ============================
fig, axes = plt.subplots(1, 3, figsize=(18, 6))

# Panel 1: Observation with high temperature regions outlined in red.
im0 = axes[0].imshow(obs_rep_2022, origin="lower", cmap="viridis")
axes[0].contour(mask_rep_2022, colors='red', linewidths=1)
axes[0].set_title(f"Observation (BC) @ {rep_time_str}\nRed contour: >96th percentile")
plt.colorbar(im0, ax=axes[0])

# Panel 2: Ensemble Mean Prediction.
im1 = axes[1].imshow(pred_rep_2022, origin="lower", cmap="viridis")
axes[1].set_title(f"Prediction (Ensemble Mean, BC) @ {rep_time_str}")
plt.colorbar(im1, ax=axes[1])

# Panel 3: Error (Observation - Prediction).
im2 = axes[2].imshow(diff_rep_2022, origin="lower", cmap="RdBu_r", vmin=vmin_rep_2022, vmax=vmax_rep_2022)
axes[2].set_title("Error (Observation - Prediction)")
plt.colorbar(im2, ax=axes[2])

plt.suptitle("GenCast Evaluation for High Temperature Region (BC, 2022)\nThreshold = {:.2f} K (96th percentile)".format(threshold_96_2022))
plt.tight_layout()
plt.show()

"""# Summary of the 2022 BC evaluation

Summary of Results:

1. Full Region (BC):

Ensemble Mean MAE is ~10 K, RMSE ~12 K, with a bias of about -10 K (i.e., the model underpredicts by roughly 10 K).

2. Top 40% Region:

Using the 60th percentile as the threshold, the Ensemble Mean shows an MAE of ~13.75 K, RMSE of ~15.05 K, and a bias of about -13.74 K, indicating significantly higher errors and underprediction in high-temperature areas.

3. Top 4% Region:

For the most extreme high temperatures (96th percentile), the Ensemble Mean’s errors increase further, with an MAE of ~18.95 K, RMSE of ~19.12 K, and bias of about -18.95 K.

Conclusion:

GenCast predicts the overall BC temperature field reasonably, but its performance degrades in high-temperature extremes. The model tends to underpredict extreme heat, with errors increasing as you focus on the hottest 40% and 4% of the region.

# Generate Comparable result table for both years
"""

import pandas as pd

# Create a dictionary of metrics.

metrics_data = {
    "Region": ["Full", "Top 40%", "Top 4%"],
    "2019 MAE (K)": [mae_full_2019, mae_40_2019, mae_4_2019],
    "2019 RMSE (K)": [rmse_full_2019, rmse_40_2019, rmse_4_2019],
    "2019 Bias (K)": [bias_full_2019, bias_40_2019, bias_4_2019],
    "2022 MAE (K)": [mae_full_2022, mae_40_2022, mae_4_2022],
    "2022 RMSE (K)": [rmse_full_2022, rmse_40_2022, rmse_4_2022],
    "2022 Bias (K)": [bias_full_2022, bias_40_2022, bias_4_2022],
    "2019 CRPS (K)": [mean_crps_2019, None, None],
    "2022 CRPS (K)": [None, None, mean_crps_2022]
}

# Create a DataFrame from the dictionary.
df_metrics = pd.DataFrame(metrics_data)

# Print the table.
print(df_metrics)

"""# Train the model

The following operations requires larger amounts of memory than running inference.

The first time executing the cell takes more time, as it includes the time to jit the function.
"""

# @title Loss computation
loss, diagnostics = loss_fn_jitted(
    jax.random.PRNGKey(0),
    train_inputs,
    train_targets,
    train_forcings)
print("Loss:", float(loss))

# @title Gradient computation
loss, diagnostics, next_state, grads = grads_fn_jitted(
    params=params,
    state=state,
    inputs=train_inputs,
    targets=train_targets,
    forcings=train_forcings)
mean_grad = np.mean(jax.tree_util.tree_flatten(jax.tree_util.tree_map(lambda x: np.abs(x).mean(), grads))[0])
print(f"Loss: {loss:.4f}, Mean |grad|: {mean_grad:.6f}")

"""# Other definitions?"""

import xarray as xr
import numpy as np

# 使用本地文件路径加载数据
file_path = "/content/gencast_dataset_source-era5_date-2019-03-29_res-0.25_levels-13_steps-01.nc"
example_batch = xr.open_dataset(file_path)

print("=== Full Dataset Information ===")
print(example_batch)

# 检查数据集中是否包含变量 "2m_temperature"
if "2m_temperature" in example_batch.data_vars:
    obs_data = example_batch["2m_temperature"]
    print("\n=== Observed Data (2m_temperature) ===")
    print(obs_data)

    print("\n=== Dimensions and Sizes ===")
    for dim, size in obs_data.sizes.items():
        print(f"{dim}: {size}")

    print("\n=== Coordinate Details ===")
    for coord in obs_data.coords:
        print(f"{coord}: {obs_data.coords[coord].values}")

    # 打印时间坐标信息（如果存在）
    if "time" in obs_data.coords:
        time_values = obs_data.time.values
        print("\n=== Time Coordinate ===")
        print(time_values)

        # 计算并打印相邻时间点之间的时间差（原始值）
        time_diffs = np.diff(time_values)
        print("\n=== Raw Time Differences between Consecutive Points ===")
        print(time_diffs)

        # 将时间差转换为小时（假设time为datetime64类型，否则可能需要调整）
        time_diffs_hours = (time_diffs / np.timedelta64(1, 'h')).astype(float)
        print("\nTime Differences (in hours):")
        print(time_diffs_hours)
else:
    print("Variable '2m_temperature' is not found in the dataset.")

import numpy as np
import xarray as xr
import matplotlib.pyplot as plt
import pandas as pd

# ========== Step 1: Extract 2022 Observation and Prediction Data ==========
# Assume eval_targets_new contains the 2022 observation data
obs_2022 = eval_targets_new["2m_temperature"].isel(batch=0)  # dimensions: time, lat, lon

# For prediction, if predictions_new is a Dataset, extract "2m_temperature"
if isinstance(predictions_new, xr.Dataset):
    pred_data_2022 = predictions_new["2m_temperature"]
else:
    pred_data_2022 = predictions_new
# Compute ensemble mean (averaging over 'sample') and select the first batch
ens_pred_2022 = pred_data_2022.mean(dim="sample").isel(batch=0)  # dimensions: time, lat, lon

# ========== Step 2: Crop to BC Region ==========
# Define BC region: lat 48-60, lon 221-246 (assuming lon in 0-360)
lat_min, lat_max = 48, 60
lon_min, lon_max = 221, 246

obs_2022_BC = obs_2022.sel(lat=slice(lat_min, lat_max), lon=slice(lon_min, lon_max))
pred_2022_BC = ens_pred_2022.sel(lat=slice(lat_min, lat_max), lon=slice(lon_min, lon_max))

print("2022 BC observation shape:", obs_2022_BC.shape)
print("2022 BC prediction shape:", pred_2022_BC.shape)

# ========== Step 3: Compute the 96th Percentile Threshold ==========
# Use the 96th percentile of the BC region's observed temperature as the threshold
threshold_96_2022 = np.nanquantile(obs_2022_BC.values, 0.96)
print("2022 BC 96th percentile threshold:", threshold_96_2022)

# ========== Step 4: Choose a Representative Time Step ==========
# Here, we choose the first time step as the representative moment.
rep_time_2022 = obs_2022_BC.time.values[0]
# Since rep_time_2022 is of type timedelta64, convert it to a string for display.
rep_time_str = str(rep_time_2022)
print("Representative time:", rep_time_str)

obs_rep_2022 = obs_2022_BC.isel(time=0)
pred_rep_2022 = pred_2022_BC.isel(time=0)
diff_rep_2022 = obs_rep_2022 - pred_rep_2022

# Compute the high temperature mask for the representative time step
mask_rep_2022 = obs_rep_2022 > threshold_96_2022

# Determine a symmetric range for the error (difference) plot
max_abs_rep_2022 = np.nanmax(np.abs(diff_rep_2022.values))
vmin_rep_2022, vmax_rep_2022 = -max_abs_rep_2022, max_abs_rep_2022

# ========== Step 5: Plot the Results ==========
fig, axes = plt.subplots(1, 3, figsize=(18, 6))

# Panel 1: Observation with high temperature regions outlined in red
im0 = axes[0].imshow(obs_rep_2022, origin="lower", cmap="viridis")
axes[0].contour(mask_rep_2022, colors='red', linewidths=1)
axes[0].set_title(f"Observation (BC) @ {rep_time_str}\nRed contour: >96th percentile")
plt.colorbar(im0, ax=axes[0])

# Panel 2: Ensemble Mean Prediction
im1 = axes[1].imshow(pred_rep_2022, origin="lower", cmap="viridis")
axes[1].set_title(f"Prediction (Ensemble Mean, BC) @ {rep_time_str}")
plt.colorbar(im1, ax=axes[1])

# Panel 3: Error (Observation - Prediction)
im2 = axes[2].imshow(diff_rep_2022, origin="lower", cmap="RdBu_r", vmin=vmin_rep_2022, vmax=vmax_rep_2022)
axes[2].set_title("Error (Observation - Prediction)")
plt.colorbar(im2, ax=axes[2])

plt.suptitle("GenCast Evaluation for High Temperature Region (BC, 2022)\nThreshold = {:.2f} K (96th percentile)".format(threshold_96_2022))
plt.tight_layout()
plt.show()

